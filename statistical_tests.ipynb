{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete Statistical Analysis Results:\n",
            "==================================================\n",
            "                                                                                 0\n",
            "Comparison                            Signorino_Baseline vs Quantum_Like_Signorino\n",
            "Sample_Size                                                                    500\n",
            "Significance_Level                                                            0.05\n",
            "Signorino_Baseline_Accuracy                                                  0.788\n",
            "Quantum_Like_Signorino_Accuracy                                              0.782\n",
            "Accuracy_Difference                                                         -0.006\n",
            "Accuracy_Diff_95CI_Lower                                                    -0.056\n",
            "Accuracy_Diff_95CI_Upper                                                     0.044\n",
            "Both_Correct                                                                   311\n",
            "Model1_Only_Correct                                                             83\n",
            "Model2_Only_Correct                                                             80\n",
            "Both_Incorrect                                                                  26\n",
            "Discordant_Pairs                                                               163\n",
            "Test_Used                                                       Asymptotic McNemar\n",
            "McNemar_Statistic                                                            0.025\n",
            "P_Value_Asymptotic                                                        0.875519\n",
            "P_Value_Exact                                                             0.875581\n",
            "P_Value_Reported                                                          0.875519\n",
            "Significant                                                                  False\n",
            "Odds_Ratio                                                                   1.038\n",
            "OR_95CI_Lower                                                                0.763\n",
            "OR_95CI_Upper                                                                 1.41\n",
            "Cohens_g                                                                     0.235\n",
            "Power                                                                        0.042\n",
            "Model1_Wins                                                                     83\n",
            "Model2_Wins                                                                     80\n",
            "Model2_Better                                                                False\n",
            "Effect_Size_Interpretation                                           Medium effect\n",
            "Statistical_Conclusion           No statistically significant difference (p = 0...\n",
            "Practical_Conclusion             No evidence of systematic difference in model ...\n",
            "\n",
            "\n",
            "Publication-Ready Table:\n",
            "==============================\n",
            "                                  Comparison   N  Signorino_Baseline_Accuracy  Quantum_Like_Signorino_Accuracy  Δ Accuracy  Discordant Pairs  McNemar's χ²  p-value  Odds Ratio   Effect Size                                                            Result\n",
            "Signorino_Baseline vs Quantum_Like_Signorino 500                        0.788                            0.782      -0.006               163         0.025 0.875519       1.038 Medium effect No statistically significant difference (p = 0.875519 ≥ α = 0.05)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "import warnings\n",
        "from typing import Union, Tuple, Dict, Any\n",
        "\n",
        "def mcnemar_analysis_from_csv(\n",
        "    model1_csv: str,\n",
        "    model2_csv: str,\n",
        "    model1_name: str = \"Model_1\",\n",
        "    model2_name: str = \"Model_2\",\n",
        "    predicted_col: str = \"predicted\",\n",
        "    groundtruth_col: str = \"groundtruth\",\n",
        "    alpha: float = 0.05,\n",
        "    exact_threshold: int = 25,\n",
        "    return_raw_data: bool = False\n",
        ") -> Union[pd.DataFrame, Tuple[pd.DataFrame, Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Perform comprehensive McNemar's test analysis comparing two models from CSV files.\n",
        "    \n",
        "    This function is designed for research paper reporting and includes all necessary\n",
        "    statistical measures, effect sizes, and interpretations.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model1_csv : str\n",
        "        Path to CSV file with Model 1 results (columns: predicted, groundtruth)\n",
        "    model2_csv : str  \n",
        "        Path to CSV file with Model 2 results (columns: predicted, groundtruth)\n",
        "    model1_name : str, default \"Model_1\"\n",
        "        Name for Model 1 (for reporting)\n",
        "    model2_name : str, default \"Model_2\"\n",
        "        Name for Model 2 (for reporting)\n",
        "    predicted_col : str, default \"predicted\"\n",
        "        Name of the prediction column in CSV files\n",
        "    groundtruth_col : str, default \"groundtruth\"\n",
        "        Name of the ground truth column in CSV files\n",
        "    alpha : float, default 0.05\n",
        "        Significance level for statistical tests\n",
        "    exact_threshold : int, default 25\n",
        "        Threshold for using exact vs asymptotic test (based on discordant pairs)\n",
        "    return_raw_data : bool, default False\n",
        "        Whether to return raw data and intermediate calculations\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Comprehensive results table for research paper reporting\n",
        "    Optional[Dict] (if return_raw_data=True)\n",
        "        Raw data and intermediate calculations\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    results_df = mcnemar_analysis_from_csv(\n",
        "        model1_csv=\"signorino_results.csv\",\n",
        "        model2_csv=\"quantum_signorino_results.csv\", \n",
        "        model1_name=\"Signorino_Baseline\",\n",
        "        model2_name=\"Quantum_Like_Signorino\"\n",
        "    )\n",
        "    print(results_df)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load and validate data\n",
        "    try:\n",
        "        df1 = pd.read_csv(model1_csv)\n",
        "        df2 = pd.read_csv(model2_csv)\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(f\"Error reading CSV files: {e}\")\n",
        "    \n",
        "    # Validate required columns\n",
        "    required_cols = [predicted_col, groundtruth_col]\n",
        "    for col in required_cols:\n",
        "        if col not in df1.columns:\n",
        "            raise ValueError(f\"Column '{col}' not found in {model1_csv}\")\n",
        "        if col not in df2.columns:\n",
        "            raise ValueError(f\"Column '{col}' not found in {model2_csv}\")\n",
        "    \n",
        "    # Validate same length and ground truth\n",
        "    if len(df1) != len(df2):\n",
        "        raise ValueError(\"CSV files must have the same number of observations\")\n",
        "    \n",
        "    # Extract data\n",
        "    model1_pred = df1[predicted_col].values\n",
        "    model1_truth = df1[groundtruth_col].values\n",
        "    model2_pred = df2[predicted_col].values  \n",
        "    model2_truth = df2[groundtruth_col].values\n",
        "    \n",
        "    # Validate same ground truth\n",
        "    if not np.array_equal(model1_truth, model2_truth):\n",
        "        raise ValueError(\"Ground truth must be identical in both files\")\n",
        "    \n",
        "    ground_truth = model1_truth\n",
        "    n_observations = len(ground_truth)\n",
        "    \n",
        "    # Calculate correctness for each model\n",
        "    model1_correct = (model1_pred == ground_truth).astype(int)\n",
        "    model2_correct = (model2_pred == ground_truth).astype(int)\n",
        "    \n",
        "    # Calculate basic accuracies\n",
        "    model1_accuracy = np.mean(model1_correct)\n",
        "    model2_accuracy = np.mean(model2_correct)\n",
        "    accuracy_diff = model2_accuracy - model1_accuracy\n",
        "    \n",
        "    # Create 2x2 contingency table\n",
        "    a = np.sum((model1_correct == 1) & (model2_correct == 1))  # Both correct\n",
        "    b = np.sum((model1_correct == 1) & (model2_correct == 0))  # Only Model 1 correct\n",
        "    c = np.sum((model1_correct == 0) & (model2_correct == 1))  # Only Model 2 correct  \n",
        "    d = np.sum((model1_correct == 0) & (model2_correct == 0))  # Both incorrect\n",
        "    \n",
        "    # Verify contingency table sums correctly\n",
        "    assert a + b + c + d == n_observations, \"Contingency table error\"\n",
        "    \n",
        "    contingency_matrix = np.array([[a, b], [c, d]])\n",
        "    discordant_pairs = b + c\n",
        "    \n",
        "    # Determine which test to use\n",
        "    use_exact = discordant_pairs < exact_threshold\n",
        "    \n",
        "    # Perform McNemar's test\n",
        "    if discordant_pairs == 0:\n",
        "        # No discordant pairs - models perform identically\n",
        "        mcnemar_statistic = 0.0\n",
        "        p_value_asymptotic = 1.0\n",
        "        p_value_exact = 1.0\n",
        "        test_used = \"No test needed\"\n",
        "        significant = False\n",
        "    else:\n",
        "        # Asymptotic McNemar's test (with continuity correction)\n",
        "        mcnemar_statistic = (abs(b - c) - 1)**2 / discordant_pairs\n",
        "        p_value_asymptotic = 1 - stats.chi2.cdf(mcnemar_statistic, df=1)\n",
        "        \n",
        "        # Exact McNemar's test (binomial test)\n",
        "        # Use binomtest for newer scipy versions, fallback to older method\n",
        "        try:\n",
        "            p_value_exact = stats.binomtest(min(b, c), discordant_pairs, p=0.5, alternative='two-sided').pvalue\n",
        "        except AttributeError:\n",
        "            # Fallback for older scipy versions\n",
        "            try:\n",
        "                p_value_exact = stats.binom_test(min(b, c), discordant_pairs, p=0.5, alternative='two-sided')\n",
        "            except AttributeError:\n",
        "                # Manual calculation if neither function is available\n",
        "                from scipy.stats import binom\n",
        "                p_value_exact = 2 * min(\n",
        "                    binom.cdf(min(b, c), discordant_pairs, 0.5),\n",
        "                    1 - binom.cdf(min(b, c) - 1, discordant_pairs, 0.5)\n",
        "                )\n",
        "        \n",
        "        # Choose appropriate test\n",
        "        if use_exact:\n",
        "            test_used = \"Exact McNemar\"\n",
        "            p_value_reported = p_value_exact\n",
        "        else:\n",
        "            test_used = \"Asymptotic McNemar\"\n",
        "            p_value_reported = p_value_asymptotic\n",
        "            \n",
        "        significant = p_value_reported < alpha\n",
        "    \n",
        "    # Calculate effect sizes and confidence intervals\n",
        "    \n",
        "    # Odds Ratio\n",
        "    if b * c > 0:\n",
        "        odds_ratio = b / c\n",
        "        log_or = np.log(odds_ratio)\n",
        "        se_log_or = np.sqrt(1/b + 1/c)\n",
        "        or_ci_lower = np.exp(log_or - 1.96 * se_log_or)\n",
        "        or_ci_upper = np.exp(log_or + 1.96 * se_log_or)\n",
        "    else:\n",
        "        odds_ratio = np.inf if b > c else 0 if c > b else 1\n",
        "        or_ci_lower = np.nan\n",
        "        or_ci_upper = np.nan\n",
        "    \n",
        "    # Cohen's g (effect size for McNemar's test)\n",
        "    if discordant_pairs > 0:\n",
        "        cohens_g = (b - c) / np.sqrt(discordant_pairs)\n",
        "    else:\n",
        "        cohens_g = 0\n",
        "    \n",
        "    # Bootstrap confidence interval for accuracy difference\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    n_bootstrap = 10000\n",
        "    bootstrap_diffs = []\n",
        "    \n",
        "    for _ in range(n_bootstrap):\n",
        "        # Resample with replacement\n",
        "        indices = np.random.choice(n_observations, n_observations, replace=True)\n",
        "        boot_acc1 = np.mean(model1_correct[indices])\n",
        "        boot_acc2 = np.mean(model2_correct[indices])\n",
        "        bootstrap_diffs.append(boot_acc2 - boot_acc1)\n",
        "    \n",
        "    acc_diff_ci_lower = np.percentile(bootstrap_diffs, 2.5)\n",
        "    acc_diff_ci_upper = np.percentile(bootstrap_diffs, 97.5)\n",
        "    \n",
        "    # Power analysis (post-hoc)\n",
        "    if discordant_pairs > 0:\n",
        "        observed_p = max(b, c) / discordant_pairs  # Probability of the more frequent outcome\n",
        "        # Power for detecting this effect size with current sample\n",
        "        from scipy.stats import norm\n",
        "        effect_size = 2 * abs(observed_p - 0.5)  # Distance from null hypothesis\n",
        "        z_alpha = norm.ppf(1 - alpha/2)\n",
        "        z_beta = (effect_size * np.sqrt(discordant_pairs) - z_alpha)\n",
        "        power = norm.cdf(z_beta)\n",
        "    else:\n",
        "        power = np.nan\n",
        "    \n",
        "    # Create comprehensive results DataFrame\n",
        "    results = {\n",
        "        # Study Design\n",
        "        'Comparison': f\"{model1_name} vs {model2_name}\",\n",
        "        'Sample_Size': n_observations,\n",
        "        'Significance_Level': alpha,\n",
        "        \n",
        "        # Model Performance\n",
        "        f'{model1_name}_Accuracy': model1_accuracy,\n",
        "        f'{model2_name}_Accuracy': model2_accuracy, \n",
        "        'Accuracy_Difference': accuracy_diff,\n",
        "        'Accuracy_Diff_95CI_Lower': acc_diff_ci_lower,\n",
        "        'Accuracy_Diff_95CI_Upper': acc_diff_ci_upper,\n",
        "        \n",
        "        # Contingency Table\n",
        "        'Both_Correct': a,\n",
        "        'Model1_Only_Correct': b,\n",
        "        'Model2_Only_Correct': c,\n",
        "        'Both_Incorrect': d,\n",
        "        'Discordant_Pairs': discordant_pairs,\n",
        "        \n",
        "        # Statistical Test Results\n",
        "        'Test_Used': test_used,\n",
        "        'McNemar_Statistic': mcnemar_statistic,\n",
        "        'P_Value_Asymptotic': p_value_asymptotic,\n",
        "        'P_Value_Exact': p_value_exact,\n",
        "        'P_Value_Reported': p_value_reported if discordant_pairs > 0 else p_value_exact,\n",
        "        'Significant': significant,\n",
        "        \n",
        "        # Effect Sizes\n",
        "        'Odds_Ratio': odds_ratio,\n",
        "        'OR_95CI_Lower': or_ci_lower,\n",
        "        'OR_95CI_Upper': or_ci_upper,\n",
        "        'Cohens_g': cohens_g,\n",
        "        \n",
        "        # Additional Statistics\n",
        "        'Power': power,\n",
        "        'Model1_Wins': b,\n",
        "        'Model2_Wins': c,\n",
        "        'Model2_Better': c > b if discordant_pairs > 0 else False,\n",
        "        \n",
        "        # Interpretation Helpers\n",
        "        'Effect_Size_Interpretation': _interpret_effect_size(abs(cohens_g)),\n",
        "        'Statistical_Conclusion': _statistical_conclusion(significant, p_value_reported if discordant_pairs > 0 else p_value_exact, alpha),\n",
        "        'Practical_Conclusion': _practical_conclusion(accuracy_diff, significant, model2_name, model1_name)\n",
        "    }\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame([results])\n",
        "    \n",
        "    # Format numeric columns for publication\n",
        "    numeric_cols = ['Sample_Size', 'Both_Correct', 'Model1_Only_Correct', \n",
        "                   'Model2_Only_Correct', 'Both_Incorrect', 'Discordant_Pairs',\n",
        "                   'Model1_Wins', 'Model2_Wins']\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if col in results_df.columns:\n",
        "            results_df[col] = results_df[col].astype(int)\n",
        "    \n",
        "    # Round numeric columns appropriately\n",
        "    decimal_cols = {\n",
        "        'Accuracy_Difference': 4,\n",
        "        'Accuracy_Diff_95CI_Lower': 4, \n",
        "        'Accuracy_Diff_95CI_Upper': 4,\n",
        "        'McNemar_Statistic': 3,\n",
        "        'P_Value_Asymptotic': 6,\n",
        "        'P_Value_Exact': 6,\n",
        "        'P_Value_Reported': 6,\n",
        "        'Odds_Ratio': 3,\n",
        "        'OR_95CI_Lower': 3,\n",
        "        'OR_95CI_Upper': 3,\n",
        "        'Cohens_g': 3,\n",
        "        'Power': 3\n",
        "    }\n",
        "    \n",
        "    for col, decimals in decimal_cols.items():\n",
        "        if col in results_df.columns:\n",
        "            results_df[col] = results_df[col].round(decimals)\n",
        "    \n",
        "    # Format accuracy columns as percentages for display\n",
        "    accuracy_cols = [f'{model1_name}_Accuracy', f'{model2_name}_Accuracy']\n",
        "    for col in accuracy_cols:\n",
        "        if col in results_df.columns:\n",
        "            results_df[col] = results_df[col].round(4)\n",
        "    \n",
        "    if return_raw_data:\n",
        "        raw_data = {\n",
        "            'model1_predictions': model1_pred,\n",
        "            'model2_predictions': model2_pred,\n",
        "            'ground_truth': ground_truth,\n",
        "            'model1_correct': model1_correct,\n",
        "            'model2_correct': model2_correct,\n",
        "            'contingency_matrix': contingency_matrix,\n",
        "            'bootstrap_diffs': bootstrap_diffs\n",
        "        }\n",
        "        return results_df, raw_data\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "def _interpret_effect_size(cohens_g: float) -> str:\n",
        "    \"\"\"Interpret Cohen's g effect size.\"\"\"\n",
        "    if np.isnan(cohens_g) or cohens_g == 0:\n",
        "        return \"No effect\"\n",
        "    elif abs(cohens_g) < 0.2:\n",
        "        return \"Small effect\"\n",
        "    elif abs(cohens_g) < 0.5:\n",
        "        return \"Medium effect\" \n",
        "    else:\n",
        "        return \"Large effect\"\n",
        "\n",
        "def _statistical_conclusion(significant: bool, p_value: float, alpha: float) -> str:\n",
        "    \"\"\"Generate statistical conclusion text.\"\"\"\n",
        "    if significant:\n",
        "        return f\"Statistically significant difference (p = {p_value:.6f} < α = {alpha})\"\n",
        "    else:\n",
        "        return f\"No statistically significant difference (p = {p_value:.6f} ≥ α = {alpha})\"\n",
        "\n",
        "def _practical_conclusion(acc_diff: float, significant: bool, model2_name: str, model1_name: str) -> str:\n",
        "    \"\"\"Generate practical conclusion text.\"\"\"\n",
        "    if not significant:\n",
        "        return \"No evidence of systematic difference in model performance\"\n",
        "    \n",
        "    better_model = model2_name if acc_diff > 0 else model1_name\n",
        "    worse_model = model1_name if acc_diff > 0 else model2_name\n",
        "    \n",
        "    diff_pct = abs(acc_diff) * 100\n",
        "    \n",
        "    if diff_pct < 1:\n",
        "        magnitude = \"minimal\"\n",
        "    elif diff_pct < 3:\n",
        "        magnitude = \"small\"\n",
        "    elif diff_pct < 5:\n",
        "        magnitude = \"moderate\"\n",
        "    else:\n",
        "        magnitude = \"substantial\"\n",
        "    \n",
        "    return f\"{better_model} performs significantly better than {worse_model} ({magnitude} improvement: {diff_pct:.2f} percentage points)\"\n",
        "\n",
        "def create_publication_table(results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a clean table formatted for research paper publication.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Select key columns for publication\n",
        "    pub_cols = [\n",
        "        'Comparison',\n",
        "        'Sample_Size',\n",
        "        f'{results_df.iloc[0][\"Comparison\"].split(\" vs \")[0]}_Accuracy',\n",
        "        f'{results_df.iloc[0][\"Comparison\"].split(\" vs \")[1]}_Accuracy', \n",
        "        'Accuracy_Difference',\n",
        "        'Discordant_Pairs',\n",
        "        'McNemar_Statistic', \n",
        "        'P_Value_Reported',\n",
        "        'Odds_Ratio',\n",
        "        'Effect_Size_Interpretation',\n",
        "        'Statistical_Conclusion'\n",
        "    ]\n",
        "    \n",
        "    # Create publication table\n",
        "    pub_table = results_df[pub_cols].copy()\n",
        "    \n",
        "    # Rename columns for publication\n",
        "    new_names = {\n",
        "        'Sample_Size': 'N',\n",
        "        'Accuracy_Difference': 'Δ Accuracy',\n",
        "        'Discordant_Pairs': 'Discordant Pairs',\n",
        "        'McNemar_Statistic': \"McNemar's χ²\",\n",
        "        'P_Value_Reported': 'p-value',\n",
        "        'Odds_Ratio': 'Odds Ratio',\n",
        "        'Effect_Size_Interpretation': 'Effect Size',\n",
        "        'Statistical_Conclusion': 'Result'\n",
        "    }\n",
        "    \n",
        "    pub_table = pub_table.rename(columns=new_names)\n",
        "    \n",
        "    return pub_table\n",
        "\n",
        "# Example usage and testing function\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example of how to use the function with sample data.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create sample CSV files for demonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 500\n",
        "    \n",
        "    outcomes = ['ACQ1', 'ACQ2', 'CAP1', 'CAP2', 'SQ', 'NEGO', 'WAR1', 'WAR2']\n",
        "    ground_truth = np.random.choice(outcomes, n_samples)\n",
        "    \n",
        "    # Model 1: 75% accuracy\n",
        "    model1_correct = np.random.binomial(1, 0.75, n_samples)\n",
        "    model1_pred = ground_truth.copy()\n",
        "    error_indices = np.where(model1_correct == 0)[0]\n",
        "    model1_pred[error_indices] = np.random.choice(outcomes, len(error_indices))\n",
        "    \n",
        "    # Model 2: 78% accuracy  \n",
        "    model2_correct = np.random.binomial(1, 0.78, n_samples)\n",
        "    model2_pred = ground_truth.copy()\n",
        "    error_indices = np.where(model2_correct == 0)[0]\n",
        "    model2_pred[error_indices] = np.random.choice(outcomes, len(error_indices))\n",
        "    \n",
        "    # Save sample CSV files\n",
        "    pd.DataFrame({\n",
        "        'predicted': model1_pred,\n",
        "        'groundtruth': ground_truth\n",
        "    }).to_csv('model1_results.csv', index=False)\n",
        "    \n",
        "    pd.DataFrame({\n",
        "        'predicted': model2_pred, \n",
        "        'groundtruth': ground_truth\n",
        "    }).to_csv('model2_results.csv', index=False)\n",
        "    \n",
        "    # Run analysis\n",
        "    results = mcnemar_analysis_from_csv(\n",
        "        'model1_results.csv',\n",
        "        'model2_results.csv',\n",
        "        model1_name='Signorino_Baseline',\n",
        "        model2_name='Quantum_Like_Signorino'\n",
        "    )\n",
        "    \n",
        "    print(\"Complete Statistical Analysis Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(results.T)  # Transpose for better readability\n",
        "    \n",
        "    print(\"\\n\\nPublication-Ready Table:\")\n",
        "    print(\"=\" * 30)\n",
        "    pub_table = create_publication_table(results)\n",
        "    print(pub_table.to_string(index=False))\n",
        "    \n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run example\n",
        "    example_results = example_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXACT McNemar's Test Analysis Results:\n",
            "=======================================================\n",
            "                                                                                  0\n",
            "Comparison                             Signorino_Baseline vs Quantum_Like_Signorino\n",
            "Sample_Size                                                                     500\n",
            "Significance_Level                                                             0.05\n",
            "Test_Method                                                    Exact McNemar's Test\n",
            "Signorino_Baseline_Accuracy                                                   0.788\n",
            "Quantum_Like_Signorino_Accuracy                                               0.782\n",
            "Accuracy_Difference                                                          -0.006\n",
            "Accuracy_Diff_95CI_Lower                                                     -0.056\n",
            "Accuracy_Diff_95CI_Upper                                                      0.044\n",
            "Both_Correct                                                                    311\n",
            "Model1_Only_Correct                                                              83\n",
            "Model2_Only_Correct                                                              80\n",
            "Both_Incorrect                                                                   26\n",
            "Discordant_Pairs                                                                163\n",
            "Test_Used                                           Exact McNemar's Test (Binomial)\n",
            "Binomial_Statistic                                                               80\n",
            "Expected_Model1_Wins                                                           81.5\n",
            "Expected_Model2_Wins                                                           81.5\n",
            "P_Value_Exact                                                              0.875581\n",
            "Fisher_Exact_P_Value                                                       0.430144\n",
            "Significant                                                                   False\n",
            "P_Model1_Wins_Given_Disagreement                                             0.5092\n",
            "P_Model1_Wins_CI_Lower                                                       0.4298\n",
            "P_Model1_Wins_CI_Upper                                                       0.5882\n",
            "Odds_Ratio                                                                    1.038\n",
            "OR_95CI_Lower                                                                 0.763\n",
            "OR_95CI_Upper                                                                  1.41\n",
            "Cohens_g                                                                      0.235\n",
            "Power                                                                         0.033\n",
            "Effect_Size                                                                  0.0092\n",
            "Prob_Current_Or_More_Extreme                                               0.875581\n",
            "Min_Detectable_Difference                                                    0.0768\n",
            "Model1_Wins                                                                      83\n",
            "Model2_Wins                                                                      80\n",
            "Model2_Better                                                                 False\n",
            "Effect_Size_Interpretation                                            Medium effect\n",
            "Statistical_Conclusion            No statistically significant difference (exact...\n",
            "Practical_Conclusion              No evidence of systematic difference in model ...\n",
            "Exact_Test_Interpretation         Of 163 cases where models disagreed: Model 1 w...\n",
            "\n",
            "\n",
            "Exact Test Publication-Ready Table:\n",
            "========================================\n",
            "                                  Comparison   N  Signorino_Baseline_Accuracy  Quantum_Like_Signorino_Accuracy  Δ Accuracy  Disagreements  Exact p-value  P(Model1 wins | disagree)  Odds Ratio   Effect Size                                                                                                                                                                                                                 Result\n",
            "Signorino_Baseline vs Quantum_Like_Signorino 500                        0.788                            0.782      -0.006            163       0.875581                     0.5092       1.038 Medium effect Of 163 cases where models disagreed: Model 1 was correct in 83 cases, Model 2 in 80 cases. This difference is not statistically significant (exact p = 0.875581), suggesting no systematic advantage for either model.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "import warnings\n",
        "from typing import Union, Tuple, Dict, Any\n",
        "\n",
        "def exact_mcnemar_analysis_from_csv(\n",
        "    model1_csv: str,\n",
        "    model2_csv: str,\n",
        "    model1_name: str = \"Model_1\",\n",
        "    model2_name: str = \"Model_2\",\n",
        "    predicted_col: str = \"predicted\",\n",
        "    groundtruth_col: str = \"groundtruth\",\n",
        "    alpha: float = 0.05,\n",
        "    return_raw_data: bool = False\n",
        ") -> Union[pd.DataFrame, Tuple[pd.DataFrame, Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Perform comprehensive EXACT McNemar's test analysis comparing two models from CSV files.\n",
        "    \n",
        "    This function uses ONLY the exact binomial method for McNemar's test, regardless of sample size.\n",
        "    Designed for research paper reporting with all necessary statistical measures.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model1_csv : str\n",
        "        Path to CSV file with Model 1 results (columns: predicted, groundtruth)\n",
        "    model2_csv : str  \n",
        "        Path to CSV file with Model 2 results (columns: predicted, groundtruth)\n",
        "    model1_name : str, default \"Model_1\"\n",
        "        Name for Model 1 (for reporting)\n",
        "    model2_name : str, default \"Model_2\"\n",
        "        Name for Model 2 (for reporting)\n",
        "    predicted_col : str, default \"predicted\"\n",
        "        Name of the prediction column in CSV files\n",
        "    groundtruth_col : str, default \"groundtruth\"\n",
        "        Name of the ground truth column in CSV files\n",
        "    alpha : float, default 0.05\n",
        "        Significance level for statistical tests\n",
        "    return_raw_data : bool, default False\n",
        "        Whether to return raw data and intermediate calculations\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Comprehensive results table for research paper reporting (EXACT method only)\n",
        "    Optional[Dict] (if return_raw_data=True)\n",
        "        Raw data and intermediate calculations\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    results_df = exact_mcnemar_analysis_from_csv(\n",
        "        model1_csv=\"signorino_results.csv\",\n",
        "        model2_csv=\"quantum_signorino_results.csv\", \n",
        "        model1_name=\"Signorino_Baseline\",\n",
        "        model2_name=\"Quantum_Like_Signorino\"\n",
        "    )\n",
        "    print(results_df)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load and validate data\n",
        "    try:\n",
        "        df1 = pd.read_csv(model1_csv)\n",
        "        df2 = pd.read_csv(model2_csv)\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(f\"Error reading CSV files: {e}\")\n",
        "    \n",
        "    # Validate required columns\n",
        "    required_cols = [predicted_col, groundtruth_col]\n",
        "    for col in required_cols:\n",
        "        if col not in df1.columns:\n",
        "            raise ValueError(f\"Column '{col}' not found in {model1_csv}\")\n",
        "        if col not in df2.columns:\n",
        "            raise ValueError(f\"Column '{col}' not found in {model2_csv}\")\n",
        "    \n",
        "    # Validate same length and ground truth\n",
        "    if len(df1) != len(df2):\n",
        "        raise ValueError(\"CSV files must have the same number of observations\")\n",
        "    \n",
        "    # Extract data\n",
        "    model1_pred = df1[predicted_col].values\n",
        "    model1_truth = df1[groundtruth_col].values\n",
        "    model2_pred = df2[predicted_col].values  \n",
        "    model2_truth = df2[groundtruth_col].values\n",
        "    \n",
        "    # Validate same ground truth\n",
        "    if not np.array_equal(model1_truth, model2_truth):\n",
        "        raise ValueError(\"Ground truth must be identical in both files\")\n",
        "    \n",
        "    ground_truth = model1_truth\n",
        "    n_observations = len(ground_truth)\n",
        "    \n",
        "    # Calculate correctness for each model\n",
        "    model1_correct = (model1_pred == ground_truth).astype(int)\n",
        "    model2_correct = (model2_pred == ground_truth).astype(int)\n",
        "    \n",
        "    # Calculate basic accuracies\n",
        "    model1_accuracy = np.mean(model1_correct)\n",
        "    model2_accuracy = np.mean(model2_correct)\n",
        "    accuracy_diff = model2_accuracy - model1_accuracy\n",
        "    \n",
        "    # Create 2x2 contingency table\n",
        "    a = np.sum((model1_correct == 1) & (model2_correct == 1))  # Both correct\n",
        "    b = np.sum((model1_correct == 1) & (model2_correct == 0))  # Only Model 1 correct\n",
        "    c = np.sum((model1_correct == 0) & (model2_correct == 1))  # Only Model 2 correct  \n",
        "    d = np.sum((model1_correct == 0) & (model2_correct == 0))  # Both incorrect\n",
        "    \n",
        "    # Verify contingency table sums correctly\n",
        "    assert a + b + c + d == n_observations, \"Contingency table error\"\n",
        "    \n",
        "    contingency_matrix = np.array([[a, b], [c, d]])\n",
        "    discordant_pairs = b + c\n",
        "    \n",
        "    # Perform EXACT McNemar's test\n",
        "    if discordant_pairs == 0:\n",
        "        # No discordant pairs - models perform identically\n",
        "        p_value_exact = 1.0\n",
        "        test_used = \"No test needed (identical performance)\"\n",
        "        significant = False\n",
        "        binomial_statistic = 0\n",
        "        expected_wins_model1 = 0\n",
        "        expected_wins_model2 = 0\n",
        "    else:\n",
        "        # EXACT McNemar's test using binomial distribution\n",
        "        test_used = \"Exact McNemar's Test (Binomial)\"\n",
        "        \n",
        "        # Use binomtest for newer scipy versions, fallback to older method\n",
        "        try:\n",
        "            # For exact test, we test if min(b,c) follows Binomial(b+c, 0.5)\n",
        "            binomial_result = stats.binomtest(min(b, c), discordant_pairs, p=0.5, alternative='two-sided')\n",
        "            p_value_exact = binomial_result.pvalue\n",
        "        except AttributeError:\n",
        "            # Fallback for older scipy versions\n",
        "            try:\n",
        "                p_value_exact = stats.binom_test(min(b, c), discordant_pairs, p=0.5, alternative='two-sided')\n",
        "            except AttributeError:\n",
        "                # Manual calculation if neither function is available\n",
        "                from scipy.stats import binom\n",
        "                p_value_exact = 2 * min(\n",
        "                    binom.cdf(min(b, c), discordant_pairs, 0.5),\n",
        "                    1 - binom.cdf(min(b, c) - 1, discordant_pairs, 0.5)\n",
        "                )\n",
        "        \n",
        "        # Binomial test statistic (number of successes)\n",
        "        binomial_statistic = min(b, c)\n",
        "        \n",
        "        # Expected values under null hypothesis\n",
        "        expected_wins_model1 = discordant_pairs / 2\n",
        "        expected_wins_model2 = discordant_pairs / 2\n",
        "        \n",
        "        significant = p_value_exact < alpha\n",
        "    \n",
        "    # Calculate exact confidence interval for the probability\n",
        "    if discordant_pairs > 0:\n",
        "        # Exact confidence interval for P(Model 1 wins | models disagree)\n",
        "        p_model1_wins = b / discordant_pairs\n",
        "        \n",
        "        # Use beta distribution for exact binomial confidence interval\n",
        "        from scipy.stats import beta\n",
        "        alpha_level = 1 - 0.95  # for 95% CI\n",
        "        \n",
        "        if b == 0:\n",
        "            ci_lower = 0\n",
        "            ci_upper = 1 - (alpha_level/2)**(1/discordant_pairs)\n",
        "        elif b == discordant_pairs:\n",
        "            ci_lower = (alpha_level/2)**(1/discordant_pairs)\n",
        "            ci_upper = 1\n",
        "        else:\n",
        "            ci_lower = beta.ppf(alpha_level/2, b, discordant_pairs - b + 1)\n",
        "            ci_upper = beta.ppf(1 - alpha_level/2, b + 1, discordant_pairs - b)\n",
        "    else:\n",
        "        p_model1_wins = 0.5\n",
        "        ci_lower = 0.0\n",
        "        ci_upper = 1.0\n",
        "    \n",
        "    # Calculate effect sizes\n",
        "    \n",
        "    # Exact Odds Ratio with exact confidence interval\n",
        "    if b * c > 0:\n",
        "        odds_ratio = b / c\n",
        "        \n",
        "        # Exact confidence interval for odds ratio using Fisher's method\n",
        "        # This is more accurate than the log-normal approximation\n",
        "        try:\n",
        "            _, fisher_p_value = fisher_exact([[a, b], [c, d]], alternative='two-sided')\n",
        "            \n",
        "            # For exact OR confidence interval, use conditional maximum likelihood\n",
        "            # This is computationally intensive, so we'll use approximation but note it's exact test\n",
        "            log_or = np.log(odds_ratio)\n",
        "            se_log_or = np.sqrt(1/b + 1/c)\n",
        "            or_ci_lower = np.exp(log_or - 1.96 * se_log_or)\n",
        "            or_ci_upper = np.exp(log_or + 1.96 * se_log_or)\n",
        "        except:\n",
        "            or_ci_lower = np.nan\n",
        "            or_ci_upper = np.nan\n",
        "            fisher_p_value = p_value_exact\n",
        "    else:\n",
        "        odds_ratio = np.inf if b > c else 0 if c > b else 1\n",
        "        or_ci_lower = np.nan\n",
        "        or_ci_upper = np.nan\n",
        "        fisher_p_value = p_value_exact\n",
        "    \n",
        "    # Cohen's g (effect size for McNemar's test)\n",
        "    if discordant_pairs > 0:\n",
        "        cohens_g = (b - c) / np.sqrt(discordant_pairs)\n",
        "    else:\n",
        "        cohens_g = 0\n",
        "    \n",
        "    # Exact bootstrap confidence interval for accuracy difference\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    n_bootstrap = 10000\n",
        "    bootstrap_diffs = []\n",
        "    \n",
        "    for _ in range(n_bootstrap):\n",
        "        # Resample with replacement\n",
        "        indices = np.random.choice(n_observations, n_observations, replace=True)\n",
        "        boot_acc1 = np.mean(model1_correct[indices])\n",
        "        boot_acc2 = np.mean(model2_correct[indices])\n",
        "        bootstrap_diffs.append(boot_acc2 - boot_acc1)\n",
        "    \n",
        "    acc_diff_ci_lower = np.percentile(bootstrap_diffs, 2.5)\n",
        "    acc_diff_ci_upper = np.percentile(bootstrap_diffs, 97.5)\n",
        "    \n",
        "    # Power analysis for exact test\n",
        "    if discordant_pairs > 0:\n",
        "        observed_p = max(b, c) / discordant_pairs\n",
        "        effect_size = abs(observed_p - 0.5)\n",
        "        \n",
        "        # For exact binomial test, power calculation is more complex\n",
        "        # Using normal approximation for power calculation\n",
        "        from scipy.stats import norm\n",
        "        z_alpha = norm.ppf(1 - alpha/2)\n",
        "        z_beta = (effect_size * np.sqrt(discordant_pairs) - z_alpha)\n",
        "        power = norm.cdf(z_beta)\n",
        "    else:\n",
        "        power = np.nan\n",
        "        effect_size = 0\n",
        "    \n",
        "    # Additional exact statistics\n",
        "    if discordant_pairs > 0:\n",
        "        # Exact probability of observing this result or more extreme under H0\n",
        "        from scipy.stats import binom\n",
        "        prob_current_or_more_extreme = 1 - binom.cdf(max(b, c) - 1, discordant_pairs, 0.5)\n",
        "        prob_current_or_more_extreme *= 2  # Two-sided\n",
        "        \n",
        "        # Minimum detectable difference with current sample size\n",
        "        min_detectable_diff = 1.96 * np.sqrt(0.25 / discordant_pairs)\n",
        "    else:\n",
        "        prob_current_or_more_extreme = 1.0\n",
        "        min_detectable_diff = np.nan\n",
        "    \n",
        "    # Create comprehensive results DataFrame\n",
        "    results = {\n",
        "        # Study Design\n",
        "        'Comparison': f\"{model1_name} vs {model2_name}\",\n",
        "        'Sample_Size': n_observations,\n",
        "        'Significance_Level': alpha,\n",
        "        'Test_Method': \"Exact McNemar's Test\",\n",
        "        \n",
        "        # Model Performance\n",
        "        f'{model1_name}_Accuracy': model1_accuracy,\n",
        "        f'{model2_name}_Accuracy': model2_accuracy, \n",
        "        'Accuracy_Difference': accuracy_diff,\n",
        "        'Accuracy_Diff_95CI_Lower': acc_diff_ci_lower,\n",
        "        'Accuracy_Diff_95CI_Upper': acc_diff_ci_upper,\n",
        "        \n",
        "        # Contingency Table\n",
        "        'Both_Correct': a,\n",
        "        'Model1_Only_Correct': b,\n",
        "        'Model2_Only_Correct': c,\n",
        "        'Both_Incorrect': d,\n",
        "        'Discordant_Pairs': discordant_pairs,\n",
        "        \n",
        "        # Exact Test Results\n",
        "        'Test_Used': test_used,\n",
        "        'Binomial_Statistic': binomial_statistic,\n",
        "        'Expected_Model1_Wins': expected_wins_model1,\n",
        "        'Expected_Model2_Wins': expected_wins_model2,\n",
        "        'P_Value_Exact': p_value_exact,\n",
        "        'Fisher_Exact_P_Value': fisher_p_value if discordant_pairs > 0 else p_value_exact,\n",
        "        'Significant': significant,\n",
        "        \n",
        "        # Exact Probability Estimates\n",
        "        'P_Model1_Wins_Given_Disagreement': p_model1_wins,\n",
        "        'P_Model1_Wins_CI_Lower': ci_lower,\n",
        "        'P_Model1_Wins_CI_Upper': ci_upper,\n",
        "        \n",
        "        # Effect Sizes\n",
        "        'Odds_Ratio': odds_ratio,\n",
        "        'OR_95CI_Lower': or_ci_lower,\n",
        "        'OR_95CI_Upper': or_ci_upper,\n",
        "        'Cohens_g': cohens_g,\n",
        "        \n",
        "        # Additional Exact Statistics\n",
        "        'Power': power,\n",
        "        'Effect_Size': effect_size,\n",
        "        'Prob_Current_Or_More_Extreme': prob_current_or_more_extreme,\n",
        "        'Min_Detectable_Difference': min_detectable_diff,\n",
        "        'Model1_Wins': b,\n",
        "        'Model2_Wins': c,\n",
        "        'Model2_Better': c > b if discordant_pairs > 0 else False,\n",
        "        \n",
        "        # Interpretation Helpers\n",
        "        'Effect_Size_Interpretation': _interpret_effect_size(abs(cohens_g)),\n",
        "        'Statistical_Conclusion': _statistical_conclusion(significant, p_value_exact, alpha),\n",
        "        'Practical_Conclusion': _practical_conclusion(accuracy_diff, significant, model2_name, model1_name),\n",
        "        'Exact_Test_Interpretation': _exact_test_interpretation(b, c, discordant_pairs, p_value_exact, alpha)\n",
        "    }\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame([results])\n",
        "    \n",
        "    # Format numeric columns for publication\n",
        "    numeric_cols = ['Sample_Size', 'Both_Correct', 'Model1_Only_Correct', \n",
        "                   'Model2_Only_Correct', 'Both_Incorrect', 'Discordant_Pairs',\n",
        "                   'Model1_Wins', 'Model2_Wins', 'Binomial_Statistic']\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if col in results_df.columns:\n",
        "            results_df[col] = results_df[col].astype(int)\n",
        "    \n",
        "    # Round numeric columns appropriately\n",
        "    decimal_cols = {\n",
        "        'Accuracy_Difference': 4,\n",
        "        'Accuracy_Diff_95CI_Lower': 4, \n",
        "        'Accuracy_Diff_95CI_Upper': 4,\n",
        "        'Expected_Model1_Wins': 1,\n",
        "        'Expected_Model2_Wins': 1,\n",
        "        'P_Value_Exact': 6,\n",
        "        'Fisher_Exact_P_Value': 6,\n",
        "        'P_Model1_Wins_Given_Disagreement': 4,\n",
        "        'P_Model1_Wins_CI_Lower': 4,\n",
        "        'P_Model1_Wins_CI_Upper': 4,\n",
        "        'Odds_Ratio': 3,\n",
        "        'OR_95CI_Lower': 3,\n",
        "        'OR_95CI_Upper': 3,\n",
        "        'Cohens_g': 3,\n",
        "        'Power': 3,\n",
        "        'Effect_Size': 4,\n",
        "        'Prob_Current_Or_More_Extreme': 6,\n",
        "        'Min_Detectable_Difference': 4\n",
        "    }\n",
        "    \n",
        "    for col, decimals in decimal_cols.items():\n",
        "        if col in results_df.columns:\n",
        "            results_df[col] = results_df[col].round(decimals)\n",
        "    \n",
        "    # Format accuracy columns\n",
        "    accuracy_cols = [f'{model1_name}_Accuracy', f'{model2_name}_Accuracy']\n",
        "    for col in accuracy_cols:\n",
        "        if col in results_df.columns:\n",
        "            results_df[col] = results_df[col].round(4)\n",
        "    \n",
        "    if return_raw_data:\n",
        "        raw_data = {\n",
        "            'model1_predictions': model1_pred,\n",
        "            'model2_predictions': model2_pred,\n",
        "            'ground_truth': ground_truth,\n",
        "            'model1_correct': model1_correct,\n",
        "            'model2_correct': model2_correct,\n",
        "            'contingency_matrix': contingency_matrix,\n",
        "            'bootstrap_diffs': bootstrap_diffs,\n",
        "            'discordant_pairs_breakdown': {'model1_wins': b, 'model2_wins': c}\n",
        "        }\n",
        "        return results_df, raw_data\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "def _interpret_effect_size(cohens_g: float) -> str:\n",
        "    \"\"\"Interpret Cohen's g effect size.\"\"\"\n",
        "    if np.isnan(cohens_g) or cohens_g == 0:\n",
        "        return \"No effect\"\n",
        "    elif abs(cohens_g) < 0.2:\n",
        "        return \"Small effect\"\n",
        "    elif abs(cohens_g) < 0.5:\n",
        "        return \"Medium effect\" \n",
        "    else:\n",
        "        return \"Large effect\"\n",
        "\n",
        "def _statistical_conclusion(significant: bool, p_value: float, alpha: float) -> str:\n",
        "    \"\"\"Generate statistical conclusion text.\"\"\"\n",
        "    if significant:\n",
        "        return f\"Statistically significant difference (exact p = {p_value:.6f} < α = {alpha})\"\n",
        "    else:\n",
        "        return f\"No statistically significant difference (exact p = {p_value:.6f} ≥ α = {alpha})\"\n",
        "\n",
        "def _practical_conclusion(acc_diff: float, significant: bool, model2_name: str, model1_name: str) -> str:\n",
        "    \"\"\"Generate practical conclusion text.\"\"\"\n",
        "    if not significant:\n",
        "        return \"No evidence of systematic difference in model performance\"\n",
        "    \n",
        "    better_model = model2_name if acc_diff > 0 else model1_name\n",
        "    worse_model = model1_name if acc_diff > 0 else model2_name\n",
        "    \n",
        "    diff_pct = abs(acc_diff) * 100\n",
        "    \n",
        "    if diff_pct < 1:\n",
        "        magnitude = \"minimal\"\n",
        "    elif diff_pct < 3:\n",
        "        magnitude = \"small\"\n",
        "    elif diff_pct < 5:\n",
        "        magnitude = \"moderate\"\n",
        "    else:\n",
        "        magnitude = \"substantial\"\n",
        "    \n",
        "    return f\"{better_model} performs significantly better than {worse_model} ({magnitude} improvement: {diff_pct:.2f} percentage points)\"\n",
        "\n",
        "def _exact_test_interpretation(b: int, c: int, discordant_pairs: int, p_value: float, alpha: float) -> str:\n",
        "    \"\"\"Generate interpretation specific to exact test.\"\"\"\n",
        "    if discordant_pairs == 0:\n",
        "        return \"Models perform identically - no statistical test needed\"\n",
        "    \n",
        "    total_disagreements = discordant_pairs\n",
        "    model1_wins = b\n",
        "    model2_wins = c\n",
        "    \n",
        "    interpretation = f\"Of {total_disagreements} cases where models disagreed: \"\n",
        "    interpretation += f\"Model 1 was correct in {model1_wins} cases, Model 2 in {model2_wins} cases. \"\n",
        "    \n",
        "    if p_value < alpha:\n",
        "        winner = \"Model 1\" if model1_wins > model2_wins else \"Model 2\"\n",
        "        interpretation += f\"This difference is statistically significant (exact p = {p_value:.6f}), \"\n",
        "        interpretation += f\"indicating {winner} systematically outperforms when models disagree.\"\n",
        "    else:\n",
        "        interpretation += f\"This difference is not statistically significant (exact p = {p_value:.6f}), \"\n",
        "        interpretation += \"suggesting no systematic advantage for either model.\"\n",
        "    \n",
        "    return interpretation\n",
        "\n",
        "def create_exact_publication_table(results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a clean table formatted for research paper publication (exact method).\n",
        "    \"\"\"\n",
        "    \n",
        "    comparison_parts = results_df.iloc[0][\"Comparison\"].split(\" vs \")\n",
        "    model1_acc_col = f'{comparison_parts[0]}_Accuracy'\n",
        "    model2_acc_col = f'{comparison_parts[1]}_Accuracy'\n",
        "    \n",
        "    # Select key columns for publication\n",
        "    pub_cols = [\n",
        "        'Comparison',\n",
        "        'Sample_Size',\n",
        "        model1_acc_col,\n",
        "        model2_acc_col, \n",
        "        'Accuracy_Difference',\n",
        "        'Discordant_Pairs',\n",
        "        'P_Value_Exact',\n",
        "        'P_Model1_Wins_Given_Disagreement',\n",
        "        'Odds_Ratio',\n",
        "        'Effect_Size_Interpretation',\n",
        "        'Exact_Test_Interpretation'\n",
        "    ]\n",
        "    \n",
        "    # Create publication table\n",
        "    pub_table = results_df[pub_cols].copy()\n",
        "    \n",
        "    # Rename columns for publication\n",
        "    new_names = {\n",
        "        'Sample_Size': 'N',\n",
        "        'Accuracy_Difference': 'Δ Accuracy',\n",
        "        'Discordant_Pairs': 'Disagreements',\n",
        "        'P_Value_Exact': 'Exact p-value',\n",
        "        'P_Model1_Wins_Given_Disagreement': 'P(Model1 wins | disagree)',\n",
        "        'Odds_Ratio': 'Odds Ratio',\n",
        "        'Effect_Size_Interpretation': 'Effect Size',\n",
        "        'Exact_Test_Interpretation': 'Result'\n",
        "    }\n",
        "    \n",
        "    pub_table = pub_table.rename(columns=new_names)\n",
        "    \n",
        "    return pub_table\n",
        "\n",
        "# Example usage function\n",
        "def example_exact_usage():\n",
        "    \"\"\"\n",
        "    Example of how to use the exact McNemar's test function.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create sample CSV files for demonstration\n",
        "    np.random.seed(42)\n",
        "   # n_samples = 100  # Smaller sample to show exact vs asymptotic differences\n",
        "   # \n",
        "   # outcomes = ['ACQ1', 'ACQ2', 'CAP1', 'CAP2', 'SQ', 'NEGO', 'WAR1', 'WAR2']\n",
        "   # ground_truth = np.random.choice(outcomes, n_samples)\n",
        "    \n",
        "    # Model 1: 75% accuracy\n",
        "   # model1_correct = np.random.binomial(1, 0.75, n_samples)\n",
        "   # model1_pred = ground_truth.copy()\n",
        "   # error_indices = np.where(model1_correct == 0)[0]\n",
        "   # model1_pred[error_indices] = np.random.choice(outcomes, len(error_indices))\n",
        "    \n",
        "    # Model 2: 80% accuracy  \n",
        "   # model2_correct = np.random.binomial(1, 0.80, n_samples)\n",
        "   # model2_pred = ground_truth.copy()\n",
        "   # error_indices = np.where(model2_correct == 0)[0]\n",
        "   # model2_pred[error_indices] = np.random.choice(outcomes, len(error_indices))\n",
        "    \n",
        "    # Save sample CSV files\n",
        "   # pd.DataFrame({\n",
        "   #     'predicted': model1_pred,\n",
        "   #     'groundtruth': ground_truth\n",
        "   # }).to_csv('exact_model1_results.csv', index=False)\n",
        "    \n",
        "   # pd.DataFrame({\n",
        "   #     'predicted': model2_pred, \n",
        "   #     'groundtruth': ground_truth\n",
        "   # }).to_csv('exact_model2_results.csv', index=False)\n",
        "    \n",
        "    # Run EXACT analysis\n",
        "    results = exact_mcnemar_analysis_from_csv(\n",
        "        'model1_results.csv',\n",
        "        'model2_results.csv',\n",
        "        model1_name='Signorino_Baseline',\n",
        "        model2_name='Quantum_Like_Signorino'\n",
        "    )\n",
        "    \n",
        "    print(\"EXACT McNemar's Test Analysis Results:\")\n",
        "    print(\"=\" * 55)\n",
        "    print(results.T)  # Transpose for better readability\n",
        "    \n",
        "    print(\"\\n\\nExact Test Publication-Ready Table:\")\n",
        "    print(\"=\" * 40)\n",
        "    pub_table = create_exact_publication_table(results)\n",
        "    print(pub_table.to_string(index=False))\n",
        "    \n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run example\n",
        "    exact_results = example_exact_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
